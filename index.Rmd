---
title: "Bayesian score calibration for approximate models"
subtitle: ""
author: "Joshua J Bon"
institute: "Queensland University of Technology"
date: "BayesComp 2023"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "hygge", "bon-qut-campus-title.css"] #css: ["default", "default-fonts", "hygge"] #
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
       after_body: insert-logo.html

---
class: list-space

```{r setup, load_refs, include=FALSE, cache=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(RColorBrewer)
library(kableExtra)
library(purrr)
library(distributional)
library(ggdist)

library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           style = "html",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("biblio.bib", check = FALSE)

#### Helpers ####
capitalize <- function(string) {
  substr(string, 1, 1) <- toupper(substr(string, 1, 1))
  string
}

attach_name <- function(string, name){
  paste(name, "=", capitalize(string))
}


knitr::opts_chunk$set(cache = T)


```
<style>

.list-space li {
padding: 0.25cm;
}

.list-nobullet li {
  list-style-type:none;
}

</style>

.pull-left[
Joint work with 

- **Christopher Drovandi**, QUT
- **David Warne**, QUT
- **David Nott**, NUS


Groups

- **School of Mathematical Sciences**, QUT
- **Centre for Data Science**, QUT

Contact

- Twitter: **@bonstats**
- Email: **joshuajbon@gmail.com**

Paper

- https://arxiv.org/abs/2211.05357

]

.pull-right[

```{r, snow-bike, echo=FALSE, out.height="550"}
knitr::include_graphics("imgs/snow-bike.JPG")
```


]

---
class: list-space

## Motivation: Rascal likelihoods

.pull-left[
Current methods for **intractable** likelihoods:

- 10,000s of realisations of the data generating process (ABC, pseudo-marginal MCMC)
- May not target the true posterior (ABC)
- How to identify method failure?

]

.pull-right[
```{r, waiting, echo=FALSE}

knitr::include_graphics("imgs/little_rascals_our_gang_waiting.gif")

```
]


--

.center[
.content-box-red[
But we have approximate likelihoods/models available...
]]

---
class: list-space

## Motivation: Approximations everywhere

.pull-left[
**Likelihood approximations**
  - Limiting distributions
  - Whittle likelihood
  - Model simplification (e.g. SDE -> ODE)
  - Linear noise approximation
]

--

.pull-right[
**Posterior approximations**
  - Laplace
  - Variational inference
  - ABC
]

--

.pull-right[
**Sampling approximations**
  - Importance sampling
  - MCMC (MH, ULA, MALA, HMC)
]

---
class: list-space

## Motivation

- Fast approximate model $\hat\pi(\theta~\vert~y)$ known to be biased
- Correct the bias (average over $\theta$) using simulation
- Simulation is expensive


--

.pull-left[
.content-box-blue[
**Simulate the posterior fitting process**
<br>
1. Draw from prior $\bar\theta_m \sim \Pi$
2. Draw new data $\tilde{y}_m \sim P(~\cdot~\vert~\theta_m)$
3. Sample posterior $(\theta_i \vert \tilde{y}_m) \sim \hat{\pi}(~\cdot~\vert~\tilde{y}_m)$
<br>
for $m \in [1:M]$ and $i \in [1:N]$
]]

--

.pull-right[
.content-box-red[
**Bias correction**
<br><br>
1. Using pairs $(\theta_i, \{\theta_j\}_{i=1}^{N})$ estimate average bias $$\text{bias} = \mathbb{E}[\mathbb{E}(\theta \vert \tilde{y}_m) - \bar\theta_m]$$
2. Correct by transformation $$\theta^\prime = \theta - \text{bias}$$
]
]

---
class: list-space

## Motivation

.pull-left[
```{r bias-plot, echo = F}

true_mu <- 1
true_sigma <- 0.3

mu <- rnorm(9, mean = 1, sd = 0.1) + 0.4
sigma <- true_sigma + runif(9, -0.1, 0.1)

dt <- tibble(dist = dist_normal(mu = mu, sigma = sigma),
`Approx posterior` = paste0("\u03C0(\u03B8 | y[",1:9,"])"))

dist_mean <-  mean(mean(dt[["dist"]]))

ggplot(dt) + 
  stat_slab(aes(xdist = dist, colour = `Approx posterior`), slab_alpha = 0.1) +
  scale_color_discrete() +
  geom_vline(aes(xintercept = true_mu)) +
  geom_vline(aes(xintercept = dist_mean), linetype = "dashed") +
  theme_bw() +
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none") +
  scale_x_continuous("\u03B8", breaks = c(0,1,2)) +
   geom_segment(aes(x = dist_mean, y = 1, xend = true_mu, yend = 1),
                  arrow = arrow(length = unit(0.5, "cm")), colour = "red") +
  ggtitle(expression(hat(pi)~(theta~`|`~y[i])))

```
]

--

.pull-right[
**Questions**


1. Can we correct the variance?

2. Can we correct the entire distribution?

]

---
class: list-space

## Proper scoring rules

**Scoring rule** $S(U,x)$ compares probabilistic ''forecast'' $U$ for the ''true'' value $x$.

<br>
<br>

Define $S(U,V) = \mathbb{E}_{x\sim V} S(U,x)$, where $V$ is a probability measure.

<br>
<br>

.content-box-blue[
$S(U, \cdot)$ is **strictly proper** if 
- $S(U,U) \geq S(U, V)$ for all $V$ in some family, and
- equality holds if and only if $U = V$.
]

---
class: list-space

## Proper scoring rules

$$S(\hat\Pi(~\cdot~\vert~\tilde{y}_m), \bar{\theta}_m)$$

---
class: list-space

## Bayesian Score Calibration

1. **Generate**
  - Sample $(\bar\theta, \tilde{y}, \hat{\theta}_{1:N})$ $M$ times
  - (ground truth, simulated data, posterior)

2. **Optimise**
  - Find best transformation $f$
  - Using $S(f(\theta),\bar\theta)$ approximated from samples.

3. **Apply**
  - $f$ to posterior of real data
  - calculate calibration diagnostics.

---
class: list-space

## Generate

.content-box-blue[
$$
\bar\theta^{(m)} \sim \bar\Pi \quad \longrightarrow \quad \tilde{y}^{(m)} \sim P(
\:\cdot\;\vert~\bar\theta^{(m)} ) \quad \longrightarrow \quad \hat\theta^{(m)}_{1:N} \sim \hat{\Pi}(\;\cdot\;\vert \tilde{y}^{(m)})
$$
]

---
class: list-space

## Optimise

.content-box-blue[
$$f^{\star} = \underset{f \in \mathcal{F}}{\arg\max}~ \mathbb{E}\left[w(\bar\theta, \tilde{y})S\left(f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y}), \bar\theta\right) \right]$$
]

--

.content-box-red[
$$f^{\star} \approx \underset{f \in \mathcal{F}}{\arg\max}~ \sum_{m=1}^{M} w(\bar\theta^{(m)}, \tilde{y}^{(m)})S^{N}\left(f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y}), \bar\theta^{(m)}\right)$$
]

---
class: list-space

## Energy score and approximation

$$S(U, x) = \frac{1}{2}\mathbb{E}_{u,u^\prime\sim U}\Vert u - u^\prime \Vert_2^{\beta} - \mathbb{E}_{u\sim U}\Vert u - x \Vert_2^{\beta}$$
for $\beta \in (0,2)$.

$$S^{N}(U, x) = \frac{1}{N}\sum_{i=1}^{N} \left(\frac{1}{2}\Vert u - u_{k_i} \Vert_2^{\beta} - \Vert u - x \Vert_2^{\beta}\right)$$

where $u\sim U$ and $k$ is a random permutation vector.


---
class: list-space

## Apply
<!-- show theory then samples -->

**Adjusted posterior** for observed data

$$\theta = f^{\star}(\hat\theta)$$

where $\hat\theta \sim \hat\Pi(\cdot \vert y)$

**Calibration diagnostic**



---
class: list-space

## Overview

Schematic

---
class: list-space

## Theoretical justification


---
class: list-space

## What about the weights?


---

## Example 1: OU Process (limiting approximation)

```{r, ou-limit-dens, echo = F}
magick::image_read_pdf("imgs/ou-process-plot-uni-1.pdf",
                       pages = 1)
```

---

## Example 1: OU Process (limiting approximation)

.pull-left[
$\mu$
```{r, ou-mu-table-1, echo = F}
mu_dat_ou1 <- tibble::tribble(
  ~`Method`, ~`MSE`, ~`Bias`, ~`St. Dev`, ~`Coverage.(90%)`,
      "Approx-post", 1.54, 1.21, 0.22, 0,
      "Adjust-post (0)", 0.12, 0.15, 0.20, 64,
      "Adjust-post (0.5)", 0.12, 0.15, 0.23, 81,
      "Adjust-post (1)", 0.12, 0.15, 0.23, 82,
      "True-post", 0.12, -0.01, 0.26, 94
  )
knitr::kable(mu_dat_ou1, format = "html")
```
]

.pull-right[
$D$
```{r, ou-D-table-1, echo = F}
D_dat_ou1 <- tibble::tribble(
  ~`Method`, ~`MSE`, ~`Bias`, ~`St. Dev`, ~`Coverage.(90%)`,
      "Approx-post", 4.73, 0.18, 1.46, 85,
      "Adjust-post (0)", 4.83, 0.28, 1.24, 72,
      "Adjust-post (0.5)", 5.08, 0.41, 1.42, 81,
      "Adjust-post (1)", 5.13, 0.42, 1.45, 83,
      "True-post", 5.00, 0.37, 1.48, 85
  )
knitr::kable(D_dat_ou1, format = "html")
```
]

---

## Example 2: Bivariate OU Process (VI approximation)

.pull-left[
```{r, ou-vi-contour, echo = F}
out <- magick::image_read_pdf("imgs/gen-corr-ou-process-plot-contour.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

.pull-right[
**Correlation summaries**
```{r, corr-table, echo = FALSE}
corr_tab <- tribble(
  ~Method, ~Mean, ~`St. Dev.`,
"Approx-post", 0.00, 0.02,
"Adjust-post (0)", 0.18, 0.41,
"Adjust-post (0.5)", 0.37, 0.16,
"Adjust-post (1)", 0.37, 0.15,
"True-post", 0.42, 0.06
)
knitr::kable(corr_tab, format = "html")
```
]

---

## Example 2: Bivariate OU Process (VI approximation)

```{r, ou-vi-check, echo = F, fig.height=100}
out <- magick::image_read_pdf("imgs/gen-corr-ou-process-plot-calcheck.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```

---

## Example 3: Whittle likelihood

.pull-left[
```{r, ou-whittle-dens, echo = F}
out <- magick::image_read_pdf("imgs/whittle_example_theta_plot_orig.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

.pull-right[
```{r, ou-whittle-calcheck, echo = F}
out <- magick::image_read_pdf("imgs/whittle-calcheck.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

---

## Example 4: Lotka-Volterra SDE

.pull-left[
```{r, ou-lv-dens, echo = F}
out <- magick::image_read_pdf("imgs/lotka-posterior-comparison.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

.pull-right[
```{r, ou-lv-calcheck, echo = F}
out <- magick::image_read_pdf("imgs/lotka-calcheck.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

---
class: list-space

## Conclusions



--
- arXiv: https://arxiv.org/abs/2211.05357

---
class: inverse, center, middle, hide-logo

## Thank you for listening `r emo::ji("tornado")`
