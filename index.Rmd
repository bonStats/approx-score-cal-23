---
title: "Bayesian score calibration for approximate models"
subtitle: ""
author: "Joshua J Bon"
institute: "Queensland University of Technology"
date: "BayesComp 2023"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "hygge", "bon-qut-campus-title.css"] #css: ["default", "default-fonts", "hygge"] #
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
       after_body: insert-logo.html

---
class: list-space

```{r setup, load_refs, include=FALSE, cache=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(RColorBrewer)
library(kableExtra)
library(purrr)
library(distributional)
library(ggdist)

library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           style = "html",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("biblio.bib", check = FALSE)

#### Helpers ####
capitalize <- function(string) {
  substr(string, 1, 1) <- toupper(substr(string, 1, 1))
  string
}

attach_name <- function(string, name){
  paste(name, "=", capitalize(string))
}


knitr::opts_chunk$set(cache = T)


```
<style>

.list-space li {
padding: 0.25cm;
}

.list-nobullet li {
  list-style-type:none;
}

</style>

.pull-left[
Joint work with 

- **Christopher Drovandi**, QUT
- **David Warne**, QUT
- **David Nott**, NUS


Groups

- **School of Mathematical Sciences**, QUT
- **Centre for Data Science**, QUT

Contact

- Twitter: **@bonstats**
- Email: **joshuajbon@gmail.com**

Paper

- https://arxiv.org/abs/2211.05357

]

.pull-right[

```{r, snow-bike, echo=FALSE, out.height="550"}
knitr::include_graphics("imgs/snow-bike.JPG")
```


]

---
class: list-space

## Motivation: Troublesome likelihoods

**Intractable and/or computationally expensive**

Current methods: 
- 10,000s of realisations of the data generating process (ABC, pseudo-marginal MCMC)
- May not target the true posterior (ABC)
- Higher dimensions?

---
class: list-space

## Motivation: Approximations everywhere

.pull-left[
**Likelihood approximations**
  - Limiting distributions
  - Whittle likelihood
  - Model simplification (e.g. SDE -> ODE)
  - Linear noise approximation
]

--

.pull-right[
**Posterior approximations**
  - Laplace
  - Variational inference
  - ABC
]

--

.pull-right[
**Sampling approximations**
  - Importance sampling
  - MCMC (MH, ULA, MALA, HMC)
]

---
class: list-space

## Motivation


.pull-left[
**Data**: $y[i] \sim P(~\cdot~\vert~x=1)$

**Approx posterior**: $p(~x~\vert~y[i])$

- .red[Correct bias]


]

--

.pull-right[
```{r bias-plot, echo = F}

true_mu <- 1
true_sigma <- 0.3

mu <- rnorm(9, mean = 1, sd = 0.1) + 0.4
sigma <- true_sigma + runif(9, -0.1, 0.1)

dt <- tibble(dist = dist_normal(mu = mu, sigma = sigma),
`Approx posterior` = paste0("p(x | y[",1:9,"])"))

dist_mean <-  mean(mean(dt[["dist"]]))

ggplot(dt) + 
  stat_slab(aes(xdist = dist, colour = `Approx posterior`), slab_alpha = 0.1) +
  scale_color_discrete() +
  geom_vline(aes(xintercept = true_mu)) +
  geom_vline(aes(xintercept = dist_mean), linetype = "dashed") +
  theme_bw() +
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_x_continuous("x", breaks = c(0,1,2)) +
   geom_segment(aes(x = dist_mean, y = 1, xend = true_mu, yend = 1),
                  arrow = arrow(length = unit(0.5, "cm")), colour = "red")

```
]

---
class: list-space

## Motivation

- Can we correct for a distribution of parameter values?
- Can we correct the variance?
- Can we correct the entire distribution?

---
class: list-space

## Proper scoring rules

Let $U$ be a probability measure, the scoring rule $S(U,x)$ measures the accuracy of the probabilistic ''forecast'' $U$ for the ''true'' value $x$.

If $V$ is a probability measure, define $S(U,V) = \mathbb{E}_{x\sim V} S(U,x)$.

A scoring rule $S(U, \cdot)$ is *strictly proper* if $S(U,U) \geq S(U, V)$ for all $V$ in some family of probability distributions and equality holds if and only if $U = V$.

---
class: list-space

## Bayesian Score Calibration

1. **Generate**
  - Sample $(\bar\theta, \tilde{y}, \hat{\theta}_{1:N})$ 
  - (ground truth, simulated data, posterior)

2. **Optimise**
  - Find best transformation $f$
  - Using $S(f(\theta),\bar\theta)$ approximated from samples.

3. **Apply**
  - $f$ to posterior of real data
  - calculate calibration diagnostics.

---
class: list-space

## Generate

.content-box-blue[
$$
\bar\theta^{(m)} \sim \bar\Pi \quad \longrightarrow \quad \tilde{y}^{(m)} \sim P(
\:\cdot\;\vert~\bar\theta^{(m)} ) \quad \longrightarrow \quad \hat\theta^{(m)}_{1:N} \sim \hat{\Pi}(\;\cdot\;\vert \tilde{y}^{(m)})
$$
]

---
class: list-space

## Optimise

.content-box-blue[
$$f^{\star} = \underset{f \in \mathcal{F}}{\arg\max}~ \mathbb{E}\left[w(\bar\theta, \tilde{y})S\left(f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y}), \bar\theta\right) \right]$$
]

--

.content-box-red[
$$f^{\star} \approx \underset{f \in \mathcal{F}}{\arg\max}~ \sum_{m=1}^{M} w(\bar\theta^{(m)}, \tilde{y}^{(m)})S^{N}\left(f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y}), \bar\theta^{(m)}\right)$$
]

---
class: list-space

## Energy score and approximation

$$S(U, x) = \frac{1}{2}\mathbb{E}_{u,u^\prime\sim U}\Vert u - u^\prime \Vert_2^{\beta} - \mathbb{E}_{u\sim U}\Vert u - x \Vert_2^{\beta}$$
for $\beta \in (0,2)$.

$$S^{N}(U, x) = \frac{1}{N}\sum_{i=1}^{N} \left(\frac{1}{2}\Vert u - u_{k_i} \Vert_2^{\beta} - \Vert u - x \Vert_2^{\beta}\right)$$

where $u\sim U$ and $k$ is a random permutation vector.


---
class: list-space

## Apply
<!-- show theory then samples -->

**Adjusted posterior** for observed data

$$\theta = f^{\star}(\hat\theta)$$

where $\hat\theta \sim \hat\Pi(\cdot \vert y)$

**Calibration diagnostic**



---
class: list-space

## Overview

Schematic

---
class: list-space

## Theoretical justification


---
class: list-space

## What about the weights?


---
class: list-space

## Examples

---
class: list-space

## Conclusions



--
- arXiv: https://arxiv.org/abs/2211.05357

---
class: inverse, center, middle, hide-logo

## Thank you for listening `r emo::ji("tornado")`
