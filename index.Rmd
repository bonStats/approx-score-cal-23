---
title: "Bayesian score calibration for approximate models"
subtitle: ""
author: "Joshua J Bon"
institute: "Queensland University of Technology"
date: "BayesComp 2023"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "hygge", "bon-qut-campus-title.css"] #css: ["default", "default-fonts", "hygge"] #
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
       after_body: insert-logo.html

---
class: list-space

```{r setup, load_refs, include=FALSE, cache=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(RColorBrewer)
library(kableExtra)
library(purrr)
library(distributional)
library(ggdist)

library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           style = "html",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("biblio.bib", check = FALSE)

#### Helpers ####
capitalize <- function(string) {
  substr(string, 1, 1) <- toupper(substr(string, 1, 1))
  string
}

attach_name <- function(string, name){
  paste(name, "=", capitalize(string))
}


knitr::opts_chunk$set(cache = T)


```
<style>

.list-space li {
padding: 0.25cm;
}

.list-nobullet li {
  list-style-type:none;
}

</style>

.pull-left[
Joint work with 

- **Christopher Drovandi**, QUT
- **David Warne**, QUT
- **David Nott**, NUS


Groups

- **School of Mathematical Sciences**, QUT
- **Centre for Data Science**, QUT

Contact

- Twitter: **@bonstats**
- Email: **joshuajbon@gmail.com**

Paper

- https://arxiv.org/abs/2211.05357

]

.pull-right[

```{r, snow-bike, echo=FALSE, out.height="550"}
knitr::include_graphics("imgs/snow-bike.JPG")
```


]

---
class: list-space

## Motivation: Troublesome likelihoods

**Intractable and/or computationally expensive**

Current methods: 
- 10,000s of realisations of the data generating process (ABC, pseudo-marginal MCMC)
- May not target the true posterior (ABC)
- Higher dimensions?

---
class: list-space

## Motivation: Approximations everywhere

.pull-left[
**Likelihood approximations**
  - Limiting distributions
  - Whittle likelihood
  - Model simplification (e.g. SDE -> ODE)
  - Linear noise approximation
]

--

.pull-right[
**Posterior approximations**
  - Laplace
  - Variational inference
  - ABC
]

--

.pull-right[
**Sampling approximations**
  - Importance sampling
  - MCMC (MH, ULA, MALA, HMC)
]

---
class: list-space

## Motivation

.pull-left[
```{r bias-plot, echo = F}

true_mu <- 1
true_sigma <- 0.3

mu <- rnorm(9, mean = 1, sd = 0.1) + 0.25
sigma <- true_sigma + runif(9, -0.1, 0.1)

dt <- tibble(dist = dist_normal(mu = mu, sigma = sigma),
`Approx posterior` = paste0("p(x | y[",1:9,"])"))

dist_mean <-  mean(mean(dt[["dist"]]))

ggplot(dt) + 
  stat_slab(aes(xdist = dist, colour = `Approx posterior`), slab_alpha = 0.1) +
  scale_color_discrete() +
  geom_vline(aes(xintercept = true_mu)) +
  geom_vline(aes(xintercept = dist_mean), linetype = "dashed") +
  theme_bw() +
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank()) +
  scale_x_continuous("x", breaks = c(0,1,2)) +
   geom_segment(aes(x = dist_mean, y = 1, xend = true_mu, yend = 1),
                  arrow = arrow(length = unit(0.5, "cm")), colour = "red")

```
]
.pull-right[
**Data generating process**
$$y[i] \sim P(~\cdot~\vert~x=1)$$

- .red[Correct bias]
- Correct variance?
- Correct posterior?
]


---
class: list-space

## Bayesian Score Calibration

1. **Generate**
  - sample $(\bar\theta, \tilde{y}, \hat{\theta}_{1:N})$ 
  - (ground truth, simulated data, posterior)

2. **Optimise**
  - Find best transformation $f$
  - Using $S(f(\theta),\bar\theta)$ approximated from samples.

3. **Apply**
  - $f$ to posterior of real data
  - calculate calibration diagnostics.

---
class: list-space

## Generate

.content-box-blue[
$$
\bar\theta^{(m)} \sim \bar\Pi \quad \longrightarrow \quad \tilde{y}^{(m)} \sim P(
\:\cdot\;\vert~\bar\theta^{(m)} ) \quad \longrightarrow \quad \hat\theta^{(m)}_{1:N} \sim \hat{\Pi}(\;\cdot\;\vert \tilde{y}^{(m)})
$$
]

---
class: list-space

## Optimise

.content-box-blue[
$$f^{\star} = \underset{f \in \mathcal{F}}{\arg\max}~ \mathbb{E}\left[w(\bar\theta, \tilde{y})S\left(f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y}), \bar\theta\right) \right]$$
]

--

.content-box-red[
$$f^{\star} \approx \underset{f \in \mathcal{F}}{\arg\max}~ \sum_{m=1}^{M} w(\bar\theta^{(m)}, \tilde{y}^{(m)})S^{N}\left(f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y}), \bar\theta^{(m)}\right)$$
]

---
class: list-space

## Energy score and approximation

---
class: list-space

## Apply
<!-- show theory then samples -->

---
class: list-space

## Overview

Schematic

---
class: list-space

## Theoretical justification of optimisation


---
class: list-space

## What about the weights?


---
class: list-space

## Examples

---
class: list-space

## Conclusions



--
- arXiv: https://arxiv.org/abs/2211.05357

---
class: inverse, center, middle, hide-logo

## Thank you for listening `r emo::ji("tornado")`
