---
title: "Bayesian score calibration for approximate models"
subtitle: ""
author: "Joshua J Bon"
institute: "Queensland University of Technology"
date: "BayesComp 2023"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "hygge", "bon-qut-campus-title.css"] #css: ["default", "default-fonts", "hygge"] #
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
       after_body: insert-logo.html

---
class: list-space

```{r setup, load_refs, include=FALSE, cache=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(RColorBrewer)
library(kableExtra)
library(purrr)
library(distributional)
library(ggdist)

library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           style = "html",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("biblio.bib", check = FALSE)

#### Helpers ####
capitalize <- function(string) {
  substr(string, 1, 1) <- toupper(substr(string, 1, 1))
  string
}

attach_name <- function(string, name){
  paste(name, "=", capitalize(string))
}


knitr::opts_chunk$set(cache = T)


```
<style>

.list-space li {
padding: 0.25cm;
}

.list-nobullet li {
  list-style-type:none;
}

</style>

.pull-left[
Joint work with 

- **Christopher Drovandi**, QUT
- **David Warne**, QUT
- **David Nott**, NUS


Groups

- **School of Mathematical Sciences**, QUT
- **Centre for Data Science**, QUT

Contact

- Twitter: **@bonstats**
- Email: **joshuajbon@gmail.com**

Paper

- https://arxiv.org/abs/2211.05357

]

.pull-right[

```{r, snow-bike, echo=FALSE, out.height="550"}
knitr::include_graphics("imgs/snow-bike.JPG")
```


]

---
class: list-space

## Motivation: "Little rascal" likelihoods

.pull-left[
Current methods for **intractable** likelihoods:

- 10,000s of realisations of the data generating process (ABC, pseudo-marginal MCMC)
- May not target the true posterior (ABC)
- How to identify method failure?

]

.pull-right[
```{r, waiting, echo=FALSE}

knitr::include_graphics("imgs/little_rascals_our_gang_waiting.gif")

```
]


--

.center[
.content-box-red[
But we have **approximate** likelihoods/models available...
]]

---
class: list-space

## Motivation: Approximations everywhere

.pull-left[
**Likelihood approximations**
  - Limiting distributions
  - Whittle likelihood
  - Model simplification (e.g. SDE -> ODE)
  - Linear noise approximation
]

--

.pull-right[
**Posterior approximations**
  - Laplace
  - Variational inference
  - ABC
]

--

.pull-right[
**Sampling approximations**
  - Importance sampling
  - MCMC (MH, ULA, MALA, HMC)
]

---
class: list-space

## Motivation

- Fast approximate model $\hat\pi(\theta~\vert~y)$ known to be biased, data simulation is expensive
- Correct the bias (average over $\theta$) using simulation


--

.pull-left[
.content-box-blue[
**Simulate the posterior fitting process**
<br>
1. Draw from prior $\bar\theta_m \sim \Pi$
2. Draw new data $\tilde{y}_m \sim P(~\cdot~\vert~\theta_m)$
3. Sample posterior $(\theta_i \vert \tilde{y}_m) \sim \hat{\pi}(~\cdot~\vert~\tilde{y}_m)$
<br>
for $m \in [1:M]$ and $i \in [1:N]$
]]

--

.pull-right[
.content-box-red[
**Bias correction**
<br><br>
1. Using pairs $(\theta_i, \{\theta_j\}_{i=1}^{N})$ estimate average bias $$\text{bias} = \mathbb{E}[\mathbb{E}(\theta \vert \tilde{y}_m) - \bar\theta_m]$$
2. Correct by transformation $$\theta^\prime = \theta - \text{bias}$$
]
]

---
class: list-space

## Motivation

.pull-left[
```{r bias-plot, echo = F}

true_mu <- 1
true_sigma <- 0.3

mu <- rnorm(9, mean = 1, sd = 0.1) + 0.4
sigma <- true_sigma + runif(9, -0.1, 0.1)

dt <- tibble(dist = dist_normal(mu = mu, sigma = sigma),
`Approx posterior` = paste0("\u03C0(\u03B8 | y[",1:9,"])"))

dist_mean <-  mean(mean(dt[["dist"]]))

ggplot(dt) + 
  stat_slab(aes(xdist = dist, colour = `Approx posterior`), slab_alpha = 0.1) +
  scale_color_discrete() +
  geom_vline(aes(xintercept = true_mu)) +
  geom_vline(aes(xintercept = dist_mean), linetype = "dashed") +
  theme_bw() +
  theme(axis.text.y = element_blank(), 
        axis.title.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none") +
  scale_x_continuous("\u03B8", breaks = c(0,1,2)) +
   geom_segment(aes(x = dist_mean, y = 1, xend = true_mu, yend = 1),
                  arrow = arrow(length = unit(0.5, "cm")), colour = "red") +
  ggtitle(expression(hat(pi)~(theta~`|`~y[i])))

```
]

--

.pull-right[
**Questions**


1. Can we correct the variance? *Frequentist*

2. Correct coverage? *Frequentist*

3. Can we correct the entire distribution? **Bayesian**

]

---
class: list-space

## Proper scoring rules

**Scoring rule** $S(U,x)$ compares probabilistic *forecast* $U$ for the *ground truth* $x$.

Define $S(U,V) = \mathbb{E}_{x\sim V} S(U,x)$, where $V$ is a probability measure.

.content-box-blue[
$S(U, \cdot)$ is **strictly proper** if 
- $S(U,U) \geq S(U, V)$ for all $V$ in some family, and
- equality holds if and only if $U = V$.
]

--

.content-box-red[ 

**We use a proper scoring rule:**

$$\underset{f \in \mathcal{F}}{\arg\max}~\mathbb{E}_{\theta\sim\Pi}\mathbb{E}_{\tilde{y}\sim P(\cdot\vert\theta) }S(f_{\#}\hat\Pi(~\cdot~\vert~\tilde{y}), \theta)$$
]

$$~$$


---
class: list-space

## Bayesian Score Calibration

1. **Generate**
  - Sample $M$ replicates of $(\bar\theta, \tilde{y}, \hat{\theta}_{1:N})$ *in parallel*
  - Ground truth, simulated data, posterior
--

2. **Optimise**
  - Find best transformation $f$
  - Using $S\left(f_{\#}\hat\Pi(\cdot~\vert~\tilde{y}),\bar\theta\right)$ approximated from samples
--

3. **Apply**
  - $f$ to posterior of real data
  - Calculate calibration diagnostics

---
class: list-space

## 1. Generate

.content-box-blue[
$$
\bar\theta^{(m)} \sim \bar\Pi \quad \longrightarrow \quad \tilde{y}^{(m)} \sim P(
\:\cdot\;\vert~\bar\theta^{(m)} ) \quad \longrightarrow \quad \hat\theta^{(m)}_{1:N} \sim \hat{\Pi}(\;\cdot\;\vert \tilde{y}^{(m)})
$$
]

--

- $\bar\Pi$ is an importance distribution to concentrate optimisation 

- $P(\:\cdot\;\vert~\bar\theta )$ is data generating process

- $\hat{\Pi}(\;\cdot\;\vert \tilde{y})$ is our approximate model

---
class: list-space

## 2. Optimise

.content-box-blue[
$$f^{\star} = \underset{f \in \mathcal{F}}{\arg\max}~ \mathbb{E}\left[w(\bar\theta, \tilde{y})S\left(f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y}), \bar\theta\right) \right]$$
]

--

.content-box-red[
$$f^{\star} \approx \underset{f \in \mathcal{F}}{\arg\max}~ \sum_{m=1}^{M} w(\bar\theta^{(m)}, \tilde{y}^{(m)})S^{N}\left(f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y}), \bar\theta^{(m)}\right)$$
]

--

.content-box-green[
**Energy score approximation** with $u\sim f_{\#}\hat\Pi(\:\cdot\:\vert\tilde{y})$ and $k$ is a random permutation vector
$$S^{N}(U, x) = \frac{1}{N}\sum_{i=1}^{N} \left(\frac{1}{2}\Vert u - u_{k_i} \Vert_2^{\beta} - \Vert u - x \Vert_2^{\beta}\right)$$
]

$$~$$
<br>
$$~$$

---
class: list-space

## 3. Apply
<!-- show theory then samples -->

.content-box-blue[
**Adjusted posterior** for observed data $y$

$$\theta = f^{\star}(\hat\theta)\quad\text{where}~ \hat\theta \sim \hat\Pi(\cdot \vert y)$$
]

--

.content-box-red[
**Calibration diagnostic**

$$\text{CC}(\rho) = \Pr\left\{\bar\theta \in \text{Cr}(f_{\#}^{\star}\hat\Pi(\cdot ~\vert~ \tilde{y}),\rho)\right\},\quad\text{where}~\tilde{y} \sim P( \cdot ~\vert~\bar\theta)$$

for $\rho \in (0,1)$.

]

--

...with **negligible cost**: (ground truth, approximate posterior) pairs already generated.

---
class: list-space

## Theoretical contribution

- Strictly proper score function $S$ 
- Importance distribution $\bar\Pi$ on $(\Theta,\vartheta)$ with $\Pi \ll \bar\Pi$ with density $\bar\pi$
- Stability function $v:\mathsf{Y} \rightarrow [0,\infty)$, measurable under $P$ on $(\mathsf{Y}, \mathcal{Y})$
- $Q$ defined by change of measure $Q(\text{d} \tilde{y}) \propto P(\text{d} \tilde{y})v(\tilde{y})$

--


**Theorem 1**:
If $Q$ has *sufficient support* about $y$ and the family of kernels $\mathcal{K}$ is *sufficiently rich*, then the Markov kernel $U^{\star}(\cdot~\vert~\cdot)$, defined as

.content-box-blue[
$$U^{\star}(\cdot~\vert~\cdot) \equiv \underset{U(\cdot~\vert~\cdot) \in \mathcal{K}}{\arg\max}~\mathbb{E}_{\theta \sim \bar\Pi} \mathbb{E}_{\tilde{y} \sim P(\cdot ~\vert~ \theta)}\left[w(\theta, \tilde{y}) S(U(\cdot ~\vert~ \tilde{y}),\theta) \right], \quad w(\theta, \tilde{y}) = \frac{\pi(\theta)}{\bar\pi(\theta)} v(\tilde{y}),$$
]

will be equal to the true posterior for the data $y$, i.e. $U^{\star}(\cdot ~\vert~ y) = \Pi(\cdot ~\vert~ y)$.


---
class: list-space

## Theoretical contribution: Key assumptions

.content-box-blue[
**Sufficiently rich kernel family**

Let $\mathcal{K}$ be a family of non-negative kernels, $Q$ be a probability measure on $(\mathsf{Y},\mathcal{Y})$ and $\Pi(\cdot ~\vert~\tilde{y})$ be the true posterior at $\tilde{y}$. We say $\mathcal{K}$ is sufficiently rich with respect to $Q$ if $\Pi(\cdot ~\vert~\tilde{y}) \in \mathcal{K}$ for all $\tilde{y}$ in the support of $Q$.
]

--

**Approximate model calibration:** $\mathcal{K} = \{f_{\#}\hat\Pi(\cdot~\vert~\cdot):f \in \mathcal{F}\}$

--

.content-box-red[
**Sufficient support about the data**

Let $Q$ be a measure on $(\mathsf{Y},\mathcal{Y})$ with support at $y$. We say that $Q$ has sufficient support about $y$ if there exists a neighborhood $B_y$, centered at $y$, such that $B_y$ has positive probability under $Q$, that is $Q(B_y) > 0$.  
]


---
class: list-space

## What about the weights?

$$w(\theta, \tilde{y}) = \frac{\pi(\theta)}{\bar\pi(\theta)} v(\tilde{y})$$

--
- New flexibility from stabilising function $v(\tilde{y})$

--
- Hard to find $v(\tilde{y})$ such that $w(\theta, \tilde{y}) \approx 1$

--
- Harder to show finite variance (?)

--

.content-box-blue[
**Clip the weights**

$$\hat{w}(\theta, \tilde{y}) = \min\{w(\theta, \tilde{y}), q_{1-\alpha}\}$$

where $q_{1-\alpha}$ is $(1-\alpha)$ quantile of raw weights.

]

---
class: list-space

## What's the transformation?

--
.content-box-blue[
**Moment-correcting transformation**

$$f(x) = L[x - \hat{\mu}(y)] + \hat{\mu}(y)+ b$$
]

--
<br>
- Mean $\hat{\mu}(y) = \mathbb{E}(\theta),\quad \theta \sim \hat\Pi(\cdot~\vert~y)$ for $y \in \mathsf{Y}$.

--
<br><br>
- $L$ is a lower triangular matrix with positive elements on diagonal.

---

## Example 1: OU Process (limiting approximation)

.content-box-blue[
$$\text{d}X_t = \gamma (\mu - X_t) \text{d}t + \sigma\text{d}W_t$$
]

--

Observe final observation at time $T$ ($n= 100$):

--

$$X_T \sim \mathcal{N}\left(  \mu + (x_0 - \mu)e^{-\gamma T}, \frac{D}{\gamma}(1- e^{-2\gamma T}) \right)$$

--
where $D = \frac{\sigma^2}{2}$. Fix $\gamma = 2$, $T=1$, $x_0 = 10$


--
Infer $\mu$ and $D$ with **approximate likelihood** based on

$$X_\infty  \sim \mathcal{N}\left(\mu, \frac{D}{\gamma}\right)$$
---

## Example 1: OU Process (limiting approximation)

```{r, ou-limit-dens, echo = F}
magick::image_read_pdf("imgs/ou-process-plot-uni-1.pdf",
                       pages = 1)
```

$M = 100, \beta = 1, \bar\Pi = f_{\#}^{(0,2)}\hat\Pi(\cdot~\vert~y)$
---

## Example 1: OU Process (limiting approximation)

.pull-left[
$\mu$
```{r, ou-mu-table-1, echo = F}
mu_dat_ou1 <- tibble::tribble(
  ~`Method`, ~`MSE`, ~`Bias`, ~`St. Dev`, ~`Coverage (90%)`,
      "Approx-post", 1.54, 1.21, 0.22, 0,
      "Adjust-post (α=0)", 0.12, 0.15, 0.20, 64,
      "Adjust-post (α=0.5)", 0.12, 0.15, 0.23, 81,
      "Adjust-post (α=1)", 0.12, 0.15, 0.23, 82,
      "True-post", 0.12, -0.01, 0.26, 94
  )
knitr::kable(mu_dat_ou1, format = "html")
```
]

.pull-right[
$D$
```{r, ou-D-table-1, echo = F}
D_dat_ou1 <- tibble::tribble(
  ~`Method`, ~`MSE`, ~`Bias`, ~`St. Dev`, ~`Coverage (90%)`,
      "Approx-post", 4.73, 0.18, 1.46, 85,
      "Adjust-post (α=0)", 4.83, 0.28, 1.24, 72,
      "Adjust-post (α=0.5)", 5.08, 0.41, 1.42, 81,
      "Adjust-post (α=1)", 5.13, 0.42, 1.45, 83,
      "True-post", 5.00, 0.37, 1.48, 85
  )
knitr::kable(D_dat_ou1, format = "html")
```
]

<br><br>
Estimated from independent replications of the method.

---

## Example 2: Bivariate OU Process (VI approximation)

.content-box-blue[
$$\text{d}X_t = \gamma (\mu - X_t) \text{d}t + \sigma\text{d}W_t$$
$$\text{d}Y_t = \gamma (\mu - Y_t) \text{d}t + \sigma\text{d}W_t$$
$$Z_t = \rho X_t + (1-\rho)Y_t$$
]

Model $(X_t,Z_t)$ with setup as in the univariate case, $(x_0,z_0)=(5,5)$.

--

Mean-field **variational approximation**

---

## Example 2: Bivariate OU Process (VI approximation)

.pull-left[
```{r, ou-vi-contour, echo = F}
out <- magick::image_read_pdf("imgs/gen-corr-ou-process-plot-contour.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

.pull-right[
**Correlation summaries**
```{r, corr-table, echo = FALSE}
corr_tab <- tribble(
  ~Method, ~Mean, ~`St. Dev.`,
"Approx-post", 0.00, 0.02,
"Adjust-post (α=0)", 0.18, 0.41,
"Adjust-post (α=0.5)", 0.37, 0.16,
"Adjust-post (α=1)", 0.37, 0.15,
"True-post", 0.42, 0.06
)
knitr::kable(corr_tab, format = "html")
```
]

$M = 100, \beta = 1, \bar\Pi = f_{\#}^{(0,2)}\hat\Pi(\cdot~\vert~y)$

---

## Example 2: Bivariate OU Process (VI approximation)

```{r, ou-vi-check, echo = F, fig.height=100}
out <- magick::image_read_pdf("imgs/gen-corr-ou-process-plot-calcheck.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```

---

## Example 3: Whittle likelihood

Fractional ARIMA time series 

.content-box-blue[

**ARFIMA**

$$\phi(L)(1-L)^d X_t = \vartheta(L)\epsilon_t$$
$$\epsilon_t \sim \mathcal{N}(0,\sigma^2)$$
]


--
$(p=2, d=0.4, q=1)$ with $n = 15,000$ observations.


--
Whittle likelihood defines **approximate posterior**.

---

## Example 3: Whittle likelihood

.pull-left[
```{r, ou-whittle-dens, echo = F}
out <- magick::image_read_pdf("imgs/whittle_example_theta_plot_orig.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

.pull-right[
```{r, ou-whittle-calcheck, echo = F}
out <- magick::image_read_pdf("imgs/whittle-calcheck.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

$M = 100, \beta = 1, \bar\Pi = f_{\#}^{(0,2)}\hat\Pi(\cdot~\vert~y), L = \text{Diagonal}$

---

## Example 4: Lotka-Volterra SDE



---

## Example 4: Lotka-Volterra SDE

.pull-left[
```{r, ou-lv-dens, echo = F}
out <- magick::image_read_pdf("imgs/lotka-posterior-comparison.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

.pull-right[
```{r, ou-lv-calcheck, echo = F}
out <- magick::image_read_pdf("imgs/lotka-calcheck.pdf",
                       pages = 1)
magick::image_scale(out, magick::geometry_size_percent(height = 30, width = 30))
```
]

$M = 200, \beta = 1, \bar\Pi = f_{\#}^{(0,2)}\hat\Pi(\cdot~\vert~y)$

---
class: list-space

## Caveats/Limitations

--
- Some regularisation useful for dependent-multivariate transformations of posteriors

--
<br><br><br>
- Results are *targeted* towards $\bar\Pi$ 
  - finite sample approximation

--
<br><br><br>
- Even more *targeted* when manipulating the weights
  - insufficiency of moment-correcting transformation with approximate posterior


---
class: list-space

## Justification of unit weights

When we take $\alpha =1$ 

---
class: list-space

## Current and future work

--
- Apply to chemical reaction networks with coarse ABC-type approximate posterior

--
- Apply to agent-based models (e.g. SIR, biological cells) with ODE-based approximate posterior

--
- Try with Pareto-smoothed weights (Vehtari et al, 2022)

--
- Use flexibility of $v(\tilde{y})$ to obtain $w(\theta, \tilde{y}) \approx 1$ apply to control weights in a SNP-style algorithm or "...GNN via Scoring Rule Minimization" (Pacchiardi & Dutta, 2022)



---
class: inverse, center, middle, hide-logo

## Thank you for listening `r emo::ji("snowboarder")`
<br><br><br><br>
https://arxiv.org/abs/2211.05357
